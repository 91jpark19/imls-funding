{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec\n",
    "import pandas as pd\n",
    "import numpy as npb\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel('/Volumes/T7/UIUC/Summer_2023/IMLS/STEM programming coding - Goals Activities Outcomes - SHARE.xlsx')#, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.fillna('') # fill empty cells with empty string\n",
    "data['goal']=data['Project goals']+data['Project activities/outcomes']\n",
    "data['goal']=data['goal'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(dataframe):\n",
    "    dataframe['goal_lower'] = dataframe['goal'].astype(str).str.lower()\n",
    "    dataframe['stopword'] = dataframe['goal_lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "    dataframe['punct'] = dataframe['stopword'].str.replace('[^\\w\\s]', '')\n",
    "    dataframe['lemma'] = dataframe['punct'].apply(lambda row: ' '.join([w.lemma_ for w in nlp(row)]))\n",
    "    dataframe['lemma_length'] = dataframe['lemma'].apply(lambda x: ' '.join([s for s in x.split() if s is not None and len(s) >= 3 and not re.match(r'^\\d+$', s)]))\n",
    "    dataframe['lemma_stop'] = dataframe['lemma_length'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "    dataframe['final'] = dataframe['lemma_stop'].apply(lambda row: ' '.join([w.lemma_ for w in nlp(row)]))\n",
    "    dataframe['token'] = dataframe['final'].apply(word_tokenize)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=lemmatization(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'museum design project increase accessibility flexibility virtual synchronous program k-12 educator students.the museum science boston offer free series interactive synchronous virtual learning experience introduce student stem topic virtual program school teacher allow museum utilize accessible complement traditional site program classroom lesson'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['final'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 20:45:31,589 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 20:45:31,610 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-07-11 20:45:31,735 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-07-11 20:45:32,820 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-07-11 20:45:32,823 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m=\u001b[39mTop2Vec(data[\u001b[39m'\u001b[39;49m\u001b[39mfinal\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mtolist())\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/top2vec/Top2Vec.py:666\u001b[0m, in \u001b[0;36mTop2Vec.__init__\u001b[0;34m(self, documents, min_count, topic_merge_delta, ngram_vocab, ngram_vocab_args, embedding_model, embedding_model_path, embedding_batch_size, split_documents, document_chunker, chunk_length, max_num_chunks, chunk_overlap_ratio, chunk_len_coverage_ratio, sentencizer, speed, use_corpus_file, document_ids, keep_documents, workers, tokenizer, use_embedding_model_tokenizer, umap_args, hdbscan_args, verbose)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    664\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00membedding_model\u001b[39m}\u001b[39;00m\u001b[39m is an invalid embedding model.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 666\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_topics(umap_args\u001b[39m=\u001b[39;49mumap_args, hdbscan_args\u001b[39m=\u001b[39;49mhdbscan_args, topic_merge_delta\u001b[39m=\u001b[39;49mtopic_merge_delta)\n\u001b[1;32m    668\u001b[0m \u001b[39m# initialize document indexing variables\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/top2vec/Top2Vec.py:1272\u001b[0m, in \u001b[0;36mTop2Vec.compute_topics\u001b[0;34m(self, umap_args, hdbscan_args, topic_merge_delta)\u001b[0m\n\u001b[1;32m   1269\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mFinding topics\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1271\u001b[0m \u001b[39m# create topic vectors\u001b[39;00m\n\u001b[0;32m-> 1272\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_topic_vectors(cluster\u001b[39m.\u001b[39;49mlabels_)\n\u001b[1;32m   1274\u001b[0m \u001b[39m# deduplicate topics\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deduplicate_topics(topic_merge_delta)\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/top2vec/Top2Vec.py:816\u001b[0m, in \u001b[0;36mTop2Vec._create_topic_vectors\u001b[0;34m(self, cluster_labels)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39min\u001b[39;00m unique_labels:\n\u001b[1;32m    814\u001b[0m     unique_labels\u001b[39m.\u001b[39mremove(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    815\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtopic_vectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_l2_normalize(\n\u001b[0;32m--> 816\u001b[0m     np\u001b[39m.\u001b[39;49mvstack([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_vectors[np\u001b[39m.\u001b[39;49mwhere(cluster_labels \u001b[39m==\u001b[39;49m label)[\u001b[39m0\u001b[39;49m]]\n\u001b[1;32m    817\u001b[0m               \u001b[39m.\u001b[39;49mmean(axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m label \u001b[39min\u001b[39;49;00m unique_labels]))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/py3.10/lib/python3.10/site-packages/numpy/core/shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    281\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "model=Top2Vec(data['final'].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
